{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DANAM Image Metadata Notebook\n",
    "This notebook is used along with the scripts clean_json and write_csv to query and analyze DANAM's image metadata quickly.\n",
    "\n",
    "Queries is done via Pandas Dataframe.\n",
    "\n",
    "DataFrame can be checked using VSCode's variable viewer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "\n",
    "#from datetime import datetime\n",
    "\n",
    "from scripts.clean_json import clean_json\n",
    "from scripts.write_csv import list_from_txt\n",
    "from scripts.metadata_fix import manual_fixes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read DANAM json export\n",
    "Always replace with the latest export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read DANAM json export\n",
    "danam_export = \"json\\DANAM\\Monument_2022-04-27_04-41-59.json\"\n",
    "danam_images = clean_json(danam_export)\n",
    "danam_df = pd.DataFrame(danam_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Review metadata extracted from DANAM before uploading and replace faulty metadata using a CSV file\n",
    "\n",
    "    df = Pandas DataFrame containing metadata of select images from DANAM\n",
    "    fixes = a CSV file containing fixes for the metadata. This CSV file has the format \n",
    "            row_number,column_name,correct_value\n",
    "    checked = if False, takes select columns to prepare metadata for review using variable viewer. \n",
    "              if True, metadata is fixed using prepared CSV defined in fixes.\n",
    "'''\n",
    "def check_metadata(df, fixes, checked):\n",
    "    cols = [\n",
    "        'danam_caption', 'caption', 'date', 'date3', 'agent', 'role', 'agent2', 'role2', 'source', 'notes', 'agent3', 'date_scan',\n",
    "        ]\n",
    "    manual_fixes(df, fixes)\n",
    "    if not checked:\n",
    "        df = df[cols]\n",
    "        print(\"PLEASE CHECK USING VARIABLE VIEW\")\n",
    "    else: \n",
    "        print(\"READY TO UPLOAD\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Metadata Uploads\n",
    "### Metadata of monuments in upload_current.mon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "READY TO UPLOAD\n"
     ]
    }
   ],
   "source": [
    "# find recently updated monuments\n",
    "mon_ids = list_from_txt('mon/upload_all.mon')\n",
    "\n",
    "# Filter metadata according to current.mon and valid caption\n",
    "upload_all = danam_df.loc[danam_df['mon_id'].isin(mon_ids)]\n",
    "upload_all = upload_all.loc[upload_all['validCaption']]\n",
    "\n",
    "\n",
    "## manual fixes start ##\n",
    "# set to True if ready to upload\n",
    "# set to False to review metadata before uploading\n",
    "upload_all = check_metadata(upload_all,\"fixes\\\\all.fix\",True)\n",
    "## manual fixes end ##\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata of recently updloaded maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "READY TO UPLOAD\n"
     ]
    }
   ],
   "source": [
    "only_maps = list_from_txt(\"mon\\\\upload_only_maps.mon\")\n",
    "\n",
    "upload_map = danam_df.loc[danam_df['mon_id'].isin(only_maps)]\n",
    "upload_map = upload_map.loc[upload_map['validCaption']]\n",
    "upload_map = upload_map.loc[upload_map['filename'].str.contains(\"_D_\")]\n",
    "\n",
    "## manual fixes start ##\n",
    "upload_map = check_metadata(upload_map,\"fixes\\\\maps.fix\",True)\n",
    "## manual fixes end ##\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata of only recently uploaded images (no maps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "READY TO UPLOAD\n"
     ]
    }
   ],
   "source": [
    "only_images = list_from_txt(\"mon\\\\upload_only_images.mon\")\n",
    "\n",
    "upload_images = danam_df.loc[danam_df['mon_id'].isin(only_images)]\n",
    "upload_images = upload_images.loc[upload_images['validCaption']]\n",
    "\n",
    "# Set this query to include everything from a monument BUT maps \n",
    "upload_images = upload_images.loc[upload_images['filename'].str.contains(\"_D_\") == False]\n",
    "\n",
    "## manual fixes start ##\n",
    "upload_images = check_metadata(upload_images,\"fixes\\\\images.fix\",True)\n",
    "## manual fixes end ##\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata of recently updloaded historical images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "READY TO UPLOAD\n"
     ]
    }
   ],
   "source": [
    "only_historical = list_from_txt(\"mon\\\\upload_only_historical.mon\")\n",
    "\n",
    "upload_historical = danam_df.loc[danam_df['mon_id'].isin(only_historical)]\n",
    "upload_historical = upload_historical.loc[upload_historical['validCaption']]\n",
    "upload_historical = upload_historical.loc[upload_historical['filename'].str.contains(\"_H_\")]\n",
    "\n",
    "## manual fixes start ##\n",
    "upload_historical = check_metadata(upload_historical,\"fixes\\\\historical.fix\",True)\n",
    "## manual fixes end ##\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding recently updated monuments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of recently updated monuments: 271\n",
      "Number of those monuments already uploaded to HeidIcon that are not in current.mon: 24\n",
      "READY TO UPLOAD\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "recent = danam_df.loc[danam_df['lastModified'] >= datetime(2022, 4, 15)]\n",
    "print(\"Number of recently updated monuments: {}\".format(recent.shape[0]))\n",
    "recent_mon_ids = set(list(set(recent['mon_id'])))\n",
    "\n",
    "uploaded = list_from_txt('mon\\\\sds.mon')\n",
    "to_update_mon = [mon for mon in recent_mon_ids if mon in uploaded and mon not in mon_ids and mon not in only_maps and mon not in only_historical]\n",
    "print(\"Number of those monuments already uploaded to HeidIcon that are not in current.mon: {}\".format(len(to_update_mon)))\n",
    "\n",
    "file = open(\"mon\\\\recently_changed.mon\", 'w')\n",
    "for mon_id in to_update_mon:\n",
    "    file.write(mon_id+\"\\n\")\n",
    "file.close()\n",
    "\n",
    "to_update = danam_df.loc[danam_df['mon_id'].isin(to_update_mon)]\n",
    "to_update = to_update.loc[to_update['validCaption']]    \n",
    "to_update = to_update.loc[to_update['lastModified'] > datetime(2022, 2, 15)]\n",
    "\n",
    "## manual fixes start ##\n",
    "to_update= check_metadata(to_update,\"fixes\\\\update.fix\",True)\n",
    "## manual fixes end ##\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting Results to CSV for Weekly Metadata Transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing to CSV \n",
    "cols = [\n",
    "        'filename', 'caption', 'date1', 'date2', 'date', 'date3', 'agent', 'role', 'agent2', 'role2',\n",
    "        'copyright', 'source', 'empty_column', 'notes', 'mon_id', 'class_code', 'classification', 'agent3', 'date_scan',\n",
    "        'license', 'url', 'rights_text', 'heidata', 'heidoc'\n",
    "        ]\n",
    "\n",
    "all_upload = pd.concat([upload_all, upload_map, upload_historical, upload_images, to_update])\n",
    "all_upload.to_csv(\"csv/image_metadata_.csv\", columns=cols, header=True, sep=';', index=False, quotechar = \"\\\"\", quoting=csv.QUOTE_ALL)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ac59ebe37160ed0dfa835113d9b8498d9f09ceb179beaac4002f036b9467c963"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
