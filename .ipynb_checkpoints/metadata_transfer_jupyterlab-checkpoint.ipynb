{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DANAM Image Metadata Notebook\n",
    "This notebook is used along with the scripts clean_json and write_csv to query and analyze DANAM's image metadata quickly.\n",
    "\n",
    "Queries is done via Pandas Dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "\n",
    "#from datetime import datetime\n",
    "\n",
    "from scripts.clean_json import clean_json\n",
    "from scripts.write_csv import list_from_txt\n",
    "from scripts.metadata_fix import manual_fixes\n",
    "\n",
    "def print_df(df):\n",
    "    cols = [\n",
    "        'danam_caption', 'caption', 'date1', 'date2', 'date', 'date3', 'agent', 'role', 'agent2', 'role2', 'source', 'notes', 'mon_id', 'class_code', 'classification', 'agent3', 'date_scan',\n",
    "        ]\n",
    "    return df[cols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Better dataframe handling with jupyter datatables\n",
    "from jupyter_datatables import init_datatables_mode\n",
    "%load_ext jupyter_require\n",
    "%requirejs d3 https://d3js.org/d3.v5.min\n",
    "init_datatables_mode()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read DANAM json export\n",
    "Always replace with the latest export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read DANAM json export\n",
    "danam_export = \"json\\DANAM\\Monument_2022-03-23_02-27-55.json\"\n",
    "danam_images = clean_json(danam_export)\n",
    "danam_df = pd.DataFrame(danam_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Metadata Uploads\n",
    "### Metadata of monuments in upload_current.mon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find recently updated monuments\n",
    "mon_ids = list_from_txt('mon/upload_current.mon')\n",
    "\n",
    "# Filter metadata according to current.mon and valid caption\n",
    "to_upload = danam_df.loc[danam_df['mon_id'].isin(mon_ids)]\n",
    "to_upload = to_upload.loc[to_upload['validCaption']]\n",
    "\n",
    "## manual fixes start ##\n",
    "fixes = \"fixes//current.fix\"\n",
    "#manual_fixes(to_upload, fixes)\n",
    "## manual fixes end ##\n",
    "\n",
    "print_df(to_upload)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata of recently updloaded maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_maps = list_from_txt(\"mon\\\\upload_only_maps.mon\")\n",
    "\n",
    "upload_map = danam_df.loc[danam_df['mon_id'].isin(only_maps)]\n",
    "upload_map = upload_map.loc[upload_map['validCaption']]\n",
    "upload_map = upload_map.loc[upload_map['filename'].str.contains(\"_D_\")]\n",
    "\n",
    "## manual fixes start ##\n",
    "fixes = \"fixes\\\\maps.fix\"\n",
    "manual_fixes(upload_map, fixes)\n",
    "## manual fixes end ##\n",
    "\n",
    "print_df(upload_map)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata of recently updloaded historical images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_historical = list_from_txt(\"mon\\\\upload_only_historical.mon\")\n",
    "\n",
    "upload_historical = danam_df.loc[danam_df['mon_id'].isin(only_historical)]\n",
    "upload_historical = upload_historical.loc[upload_historical['validCaption']]\n",
    "upload_historical = upload_historical.loc[upload_historical['filename'].str.contains(\"_H_\")]\n",
    "\n",
    "## manual fixes start ##\n",
    "fixes = \"fixes\\\\historical.fix\"\n",
    "manual_fixes(upload_historical, fixes)\n",
    "## manual fixes end ##\n",
    "\n",
    "print_df(upload_historical)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding recently updated monuments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "recent = danam_df.loc[danam_df['lastModified'] >= datetime(2022, 2, 15)]\n",
    "print(\"Number of recently update monuments: {}\".format(recent.shape[0]))\n",
    "recent_mon_ids = set(list(set(recent['mon_id'])))\n",
    "\n",
    "uploaded = list_from_txt('mon\\\\sds.mon')\n",
    "to_update_mon = [mon for mon in recent_mon_ids if mon in uploaded and mon not in mon_ids and mon not in only_maps and mon not in only_historical]\n",
    "print(\"Number of those monuments already uploaded to HeidIcon that are not in current.mon: {}\".format(len(to_update_mon)))\n",
    "\n",
    "file = open(\"mon\\\\recently_changed.mon\", 'w')\n",
    "for mon_id in to_update_mon:\n",
    "    file.write(mon_id+\"\\n\")\n",
    "file.close()\n",
    "\n",
    "to_update = danam_df.loc[danam_df['mon_id'].isin(to_update_mon)]\n",
    "to_update = to_update.loc[to_update['validCaption']]    \n",
    "to_update = to_update.loc[to_update['lastModified'] > datetime(2022, 2, 15)]\n",
    "\n",
    "## manual fixes start ##\n",
    "fixes = \"fixes\\\\update.fix\"\n",
    "manual_fixes(to_update, fixes)\n",
    "## manual fixes end ##\n",
    "\n",
    "#to_update[[ 'lastModified', 'validCaption', 'filename', 'danam_caption', 'caption', 'date', 'agent']].sort_values('lastModified', ascending=False)\n",
    "print_df(to_update)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking for caption fixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_fix = list_from_txt(\"mon\\\\upload_to_fix.mon\")\n",
    "upload_fix = danam_df.loc[danam_df['mon_id'].isin(to_fix)]\n",
    "upload_fix = upload_fix.loc[upload_fix['danam_caption'].str.contains(\"Attribution 40\")]\n",
    "\n",
    "fixes = [\"If not otherwise stated, all images and texts in this folder are published under Creative Commons\"\n",
    ", \"If not otherwise stated, all images and texts in this monument folder are published under Creative Commons\"\n",
    ", \"Attribution 4.0 License \\(CC BY-SA 4.0\\),\"\n",
    ", \"Attribution 40 License \\(CC BY-SA 40\\),\"\n",
    ", \"and the copyright lies with NHDP. All visuals of this monument folder\"\n",
    ",\" and more are \\(or will be\\) also stored in heidICON,\" \n",
    ", \"and more are also stored in heidICON,\"\n",
    ", \"the object and multimedia database of Heidelberg University\" \n",
    ", \"\\(Type the ID-number or key words in the first line and click the search field.\\)\" \n",
    ", \"\\(type the ID-number or key words in the first line and click the search field.\\)\" \n",
    ", \"\\(type the ID-number or key words in the first line and click the search field\\)\" \n",
    ", \"\\(type the ID-number or keywords in the first line and click the search field\\).\" \n",
    ", \"You will also find the initial report there\"\n",
    ", \"The latest report will always be available in DANAM \\(this page\\).\"\n",
    ", \"You will also find the initial report there. The latest report will always be available in DANAM \\(this page\\).\"\n",
    ", \".\"\n",
    "]\n",
    "\n",
    "for fix in fixes:\n",
    "    upload_fix['caption'] = upload_fix['caption'].str.replace(fix, '', regex=True, case=False)\n",
    "\n",
    "upload_fix['caption'] = upload_fix['caption'].str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting Results to CSV for Weekly Metadata Transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing to CSV \n",
    "cols = [\n",
    "        'filename', 'caption', 'date1', 'date2', 'date', 'date3', 'agent', 'role', 'agent2', 'role2',\n",
    "        'copyright', 'source', 'empty_column', 'notes', 'mon_id', 'class_code', 'classification', 'agent3', 'date_scan',\n",
    "        'license', 'url', 'rights_text', 'heidata', 'heidoc'\n",
    "        ]\n",
    "\n",
    "all_upload = pd.concat([to_upload, upload_map, upload_historical, to_update])\n",
    "all_upload.to_csv(\"csv/image_metadata_.csv\", columns=cols, header=True, sep=';', index=False, quotechar = \"\\\"\", quoting=csv.QUOTE_ALL)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ac59ebe37160ed0dfa835113d9b8498d9f09ceb179beaac4002f036b9467c963"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
